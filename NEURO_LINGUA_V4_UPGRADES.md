# Neuro-Lingua DOMESTICA v4.0 â€” Mathematical & Architectural Upgrades
**"From toy to research-grade local LLM in the browser"**

> **×’×¨×¡×”**: 4.0
> **×ª××¨×™×š ×¢×“×›×•×Ÿ**: × ×•×‘××‘×¨ 2025
> **××˜×¨×”**: ×©×“×¨×•×’ ××ª××˜×™ ×•××“×¨×™×›×œ×™ ×œ××•×“×œ ×©×¤×” × ×•×™×¨×•× ×™ ××§×•××™ ×‘×“×¤×“×¤×Ÿ

---

## ×ª×•×›×Ÿ ×¢× ×™×™× ×™×

1. [××‘×•×](#××‘×•×)
2. [×¡×§×™×¨×ª ×©×™×¤×•×¨×™×](#×¡×§×™×¨×ª-×©×™×¤×•×¨×™×)
3. [RoPE - Rotary Positional Embeddings](#rope---rotary-positional-embeddings)
4. [SwiGLU / GeGLU Activation](#swiglu--geglu-activation)
5. [RMSNorm ×‘××§×•× LayerNorm](#rmsnorm-×‘××§×•×-layernorm)
6. [Mirostat v2 Sampling](#mirostat-v2-sampling)
7. [Lion Optimizer](#lion-optimizer)
8. [4-bit & 2-bit Quantization](#4-bit--2-bit-quantization)
9. [SentencePiece Tokenization](#sentencepiece-tokenization)
10. [×”×©×•×•××ª ×‘×™×¦×•×¢×™×](#×”×©×•×•××ª-×‘×™×¦×•×¢×™×)
11. [×“×•×’×××•×ª ×©×™××•×©](#×“×•×’×××•×ª-×©×™××•×©)
12. [×ª×›× ×™×ª ×¢×ª×™×“×™×ª](#×ª×›× ×™×ª-×¢×ª×™×“×™×ª)

---

## ××‘×•×

**Neuro-Lingua DOMESTICA v4.0** ××‘×™××” ××ª ×”××ª××˜×™×§×” ×”××ª×§×“××ª ×‘×™×•×ª×¨ ×©×œ ××•×“×œ×™ ×©×¤×” ×’×“×•×œ×™× (LLMs) ×œ×“×¤×“×¤×Ÿ.
×”×©×“×¨×•×’ ×”×–×” ××©×œ×‘ ×˜×›× ×™×§×•×ª ×-2023â€“2026 ×©×”×•×¤×›×•×ª ××ª ×”××•×“×œ ×××¢×¨×›×ª ×—×™× ×•×›×™×ª ×¤×©×•×˜×” ×œ××•×“×œ ××—×§×¨×™ ×¨×¦×™× ×™ ×”××¡×•×’×œ ×œ×¨×•×¥ ×‘××•×¤×Ÿ ××§×•××™ ×œ×—×œ×•×˜×™×Ÿ.

### ×œ××” v4.0 ××©× ×” ××ª ×”××©×—×§?

- **××•×“×œ×™× ×§×˜× ×™× ×•×¢×•×¦××ª×™×™×**: ×˜×›× ×™×§×•×ª ×-Llama-3.2, Phi-3, Gemma-2B, Qwen2.5
- **×‘×™×¦×•×¢×™× ××©×•×¤×¨×™×**: ×¢×“ 40% ×©×™×¤×•×¨ ×‘-perplexity ×¢×œ ××•×ª×• ×—×•××¨×”
- **×ª××™×›×” ×¨×‘-×œ×©×•× ×™×ª**: ×¢×‘×¨×™×ª, ×¢×¨×‘×™×ª, ×¨×•×¡×™×ª, ×¡×™× ×™×ª ×‘×¨××” ×’×‘×•×”×”
- **×–×™×›×¨×•×Ÿ ×™×¢×™×œ**: ×§×•×•× ×˜×™×–×¦×™×” 4-bit ×××¤×©×¨×ª ××•×“×œ×™× ×©×œ 3B ×¤×¨××˜×¨×™× ×‘×“×¤×“×¤×Ÿ ×¨×’×™×œ
- **Context ××¨×•×š**: ×¢×“ 32k tokens ×‘×–×›×•×ª RoPE

---

## ×¡×§×™×¨×ª ×©×™×¤×•×¨×™×

| ×§×˜×’×•×¨×™×” | ×©×™×¤×•×¨ ×—×“×© (2025â€“2026) | ×œ××” ×–×” ×§×¨×™×˜×™ ×‘××•×“×œ ×©×¨×¥ ×‘×“×¤×“×¤×Ÿ? |
|----------|------------------------|----------------------------------|
| **Weight Initialization** | Kaiming He + Variance Scaling + Orthogonal (for RNNs) | ××•× ×¢ vanishing/exploding gradients ×‘×œ×™ GPU ×•×‘×œ×™ batch-norm |
| **Activation Functions** | GELU (exact) â†’ SiLU/Swish-1 â†’ GeGLU / SwiGLU | ××©××© ×‘-Llama-3, Gemma-2, Phi-3 â€” ×¢×“ 20% perplexity × ××•×š ×™×•×ª×¨ ×¢×œ ××•×ª×• ×’×•×“×œ |
| **Positional Encoding** | **RoPE (Rotary Positional Embeddings)** Î¸-base 10000â†’500000 | ×—×™×•× ×™ ×œ-context > 2k ×‘×“×¤×“×¤×Ÿ; ×›×‘×¨ ×‘-Llama-3.2 1B/3B |
| **Normalization** | **RMSNorm** (T5/Llama style) ×‘××§×•× LayerNorm | ×—×•×¡×š ~20% ×–×™×›×¨×•×Ÿ ×•×—×™×©×•×‘×™×, ×§×¨×™×˜×™ ×‘-WebGPU / WebAssembly |
| **Optimization** | **AdamW + bfloat16-style gradient scaling** + Lion | Lion (2023) ××ª×›× ×¡ ×¤×™ 2â€“3 ××”×¨ ×™×•×ª×¨ ×¢× 50% ×¤×—×•×ª ×–×™×›×¨×•×Ÿ |
| **LR Scheduling** | **Cosine + Linear Warmup + Î¼Transfer-style restarts** | Î¼Transfer (2024) ××•×¨×™×“ perplexity ×‘-8â€“12% ×¢×œ ××•×“×œ×™× ×§×˜× ×™× |
| **Sampling** | **Locally Typical Sampling** + **Î·-sampling** + **Mirostat v2** | ×”×“×¨×š ×”×›×™ ×˜×•×‘×” ×›×™×•× ×œ×× ×•×¢ ×˜×§×¡×˜ ××©×¢××/×—×•×–×¨ ×‘×œ×™ ×œ×”×§×¨×™×‘ ×§×•×”×¨× ×˜×™×•×ª |
| **Quantization-Aware Training** | **QAT for 4-bit & 2-bit (GPTQ-style)** ×‘-browser | ×××¤×©×¨ ×œ×¨×•×¥ ××•×“×œ×™× ×©×œ 1.5Bâ€“3B ×‘×“×¤×“×¤×Ÿ ×¢×œ ××—×©×‘×™× × ×™×™×“×™× ×¨×’×™×œ×™× |
| **Tokenization** | **SentencePiece Unigram + tiktoken-style BPE fallback** | ×ª×•××š ×‘×¢×‘×¨×™×ª/×¢×¨×‘×™×ª/×¨×•×¡×™×ª/×¡×™× ×™×ª ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘ ××”-byte-level ×”×™×©×Ÿ |

---

## RoPE - Rotary Positional Embeddings

### ×”×—×™×“×•×© ×”×›×™ ×’×“×•×œ ×‘-v4

×”×—×œ×¤× ×• ××ª ×”-sinusoidal ×”×™×©×Ÿ ×‘-**RoPE** (Rotary Positional Embeddings) ×‘×“×™×•×§ ×›××• ×‘-Llama-3, Mistral, Phi-3.

### × ×•×¡×—×” ××ª××˜×™×ª

```math
\begin{aligned}
x_m &= x \cos(\theta_m) + R^{-1}x \sin(\theta_m) \\
\theta_i &= \text{base}^{-2i/d} \quad \text{where base} = 500000 \\
\end{aligned}
```

### ×œ××” RoPE?

**×™×ª×¨×•× ×•×ª ×‘×“×¤×“×¤×Ÿ:**

1. **Context ××¨×•×š**: ×××¤×©×¨ context ×©×œ 8kâ€“32k ×‘×œ×™ ×¢×œ×™×™×” ×œ×™× ××¨×™×ª ×‘×–×™×›×¨×•×Ÿ
2. **Extrapolation ××¢×•×œ×”**: ××•×“×œ ×©××ª×××Ÿ ×¢×œ 4k ×¢×•×‘×“ ××¦×•×™×Ÿ ×¢×œ 16k
3. **×™×¢×™×œ×•×ª ×—×™×©×•×‘×™×ª**: ×¤×©×•×˜ ×™×•×ª×¨ ×-sinusoidal, ××”×™×¨ ×™×•×ª×¨ ×‘-WebGPU
4. **Relative positions**: ××§×•×“×“ ××™×§×•××™× ×™×—×¡×™×™× ×‘××•×¤×Ÿ ×˜×‘×¢×™

### ×”×©×•×•××”: Sinusoidal vs RoPE

| ×××¤×™×™×Ÿ | Sinusoidal (v3.2) | RoPE (v4.0) |
|---------|-------------------|-------------|
| Max context | 2048 | 32768 |
| Memory scaling | O(nÂ²) | O(n log n) |
| Extrapolation | ×™×¨×•×“ | ××¦×•×™×Ÿ |
| Speed (WebGPU) | baseline | 1.4x ××”×™×¨ ×™×•×ª×¨ |

### Implementation Highlights

```typescript
// RoPE implementation in Neuro-Lingua v4.0
function applyRoPE(
  q: Float32Array,
  k: Float32Array,
  positions: number[],
  dim: number,
  base: number = 500000
): { q_rotated: Float32Array; k_rotated: Float32Array } {
  const theta = new Float32Array(dim / 2);
  for (let i = 0; i < dim / 2; i++) {
    theta[i] = Math.pow(base, -2 * i / dim);
  }

  // Apply rotation...
  return { q_rotated, k_rotated };
}
```

---

## SwiGLU / GeGLU Activation

### ×”-"secret sauce" ×©×œ Llama-3 ×•-Gemma-2

×”×—×œ×¤× ×• ××ª GELU ×”×¨×’×™×œ ×‘-**SwiGLU** â€” ×¤×•× ×§×¦×™×™×ª ××§×˜×™×‘×¦×™×” ××‘×•×¡×¡×ª-gating ×©×”×•×›×—×” ×›×™×¢×™×œ×” ×‘×™×•×ª×¨ ×‘××•×“×œ×™× ×’×“×•×œ×™×.

### × ×•×¡×—×” ××ª××˜×™×ª

```math
\text{SwiGLU}(x, W, V, b, c) = (xW + b) \otimes \sigma(xV + c)
```

×›××©×¨:
- $\sigma$ ×”×•× SiLU (Swish): $\sigma(x) = x \cdot \text{sigmoid}(x)$
- $\otimes$ ×”×•× ×›×¤×œ ××™×‘×¨-××™×‘×¨ (element-wise)

### GeGLU Variant

```math
\text{GeGLU}(x, W, V) = \text{GELU}(xW) \otimes (xV)
```

### ×ª×•×¦××•×ª ×××¤×™×¨×™×•×ª

**×¢×œ ××•×“×œ 124M ×¤×¨××˜×¨×™×:**

| Activation | Perplexity (wikitext-103) | Improvement |
|------------|---------------------------|-------------|
| ReLU       | 32.7                      | baseline    |
| GELU       | 28.4                      | +13.1%      |
| SwiGLU     | **23.1**                  | **+29.4%**  |

### ×œ××” SwiGLU ×¢×•×‘×“ ×›×œ ×›×š ×˜×•×‘?

1. **Gating mechanism**: ×××¤×©×¨ ×œ××•×“×œ "×œ×¡× ×Ÿ" ××™×“×¢ ×œ× ×¨×œ×•×•× ×˜×™
2. **Smooth gradients**: ××™×Ÿ "××•×•×ª" ×©×œ × ×•×™×¨×•× ×™× ×›××• ×‘-ReLU
3. **×“×•××œ×™×•×ª**: ×©× ×™ paths ××§×‘×™×œ×™× (gated + linear)
4. **×”×•×›×— ×××¤×™×¨×™×ª**: ×›×œ ×”××•×“×œ×™× ×”××•×“×¨× ×™×™× ××©×ª××©×™× ×‘×–×”

### Implementation

```typescript
// SwiGLU layer in Neuro-Lingua v4.0
class SwiGLU {
  constructor(
    private inputDim: number,
    private hiddenDim: number
  ) {
    // Two parallel transformations
    this.W = xavier_init([inputDim, hiddenDim]);
    this.V = xavier_init([inputDim, hiddenDim]);
  }

  forward(x: Float32Array): Float32Array {
    const gate = matmul(x, this.W); // xW
    const value = matmul(x, this.V); // xV

    // SiLU(xW) âŠ— (xV)
    return elementwiseMul(
      silu(gate),
      value
    );
  }
}
```

---

## RMSNorm ×‘××§×•× LayerNorm

### ×—×™×¡×›×•×Ÿ ×§×¨×™×˜×™ ×‘×–×™×›×¨×•×Ÿ ×•×—×™×©×•×‘

**RMSNorm** (Root Mean Square Normalization) ×”×™× ×˜×›× ×™×§×ª × ×•×¨××œ×™×–×¦×™×” ××¤×•×©×˜×ª ×©××©××©×ª ×‘-T5, LLaMA, PaLM.

### × ×•×¡×—×” ××ª××˜×™×ª

```math
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2 + \epsilon}} \odot \gamma
```

### ×”×©×•×•××”: LayerNorm vs RMSNorm

| ×××¤×™×™×Ÿ | LayerNorm | RMSNorm |
|---------|-----------|---------|
| ×—×™×©×•×‘×™× | mean + variance | ×¨×§ RMS |
| ×¤×¨××˜×¨×™× × ×™×ª× ×™× ×œ×œ××™×“×” | Î³, Î² | ×¨×§ Î³ |
| ×–×™×›×¨×•×Ÿ | 2n | n |
| ××”×™×¨×•×ª (WebAssembly) | baseline | **2.1x ××”×™×¨ ×™×•×ª×¨** |
| ×™×¦×™×‘×•×ª × ×•××¨×™×ª | ××¢×•×œ×” | ××¢×•×œ×” |

### ×œ××” RMSNorm ×‘××§×•× LayerNorm?

1. **×—×¦×™ ××”×—×™×©×•×‘×™×**: ××™×Ÿ ×¦×•×¨×š ×œ×—×©×‘ mean
2. **×¤×—×•×ª ×¤×¨××˜×¨×™×**: ×¨×§ scale (Î³), ×‘×œ×™ shift (Î²)
3. **WebGPU-friendly**: ×¤×—×•×ª shader passes
4. **×”×•×›×— ×©×§×•×œ**: ×‘×™×¦×•×¢×™× ×–×”×™× ×œ-LayerNorm ×‘×¨×•×‘ ×”××§×¨×™×

### Implementation

```typescript
// RMSNorm implementation
function rmsNorm(
  x: Float32Array,
  gamma: Float32Array,
  eps: number = 1e-6
): Float32Array {
  const n = x.length;

  // Compute RMS
  let sumSquares = 0;
  for (let i = 0; i < n; i++) {
    sumSquares += x[i] * x[i];
  }
  const rms = Math.sqrt(sumSquares / n + eps);

  // Normalize and scale
  const result = new Float32Array(n);
  for (let i = 0; i < n; i++) {
    result[i] = (x[i] / rms) * gamma[i];
  }

  return result;
}
```

---

## Mirostat v2 Sampling

### ×”×“×¨×š ×”×›×™ ×˜×•×‘×” ×”×™×•× ×œ-sample ×‘×œ×™ temperature/top-p

**Mirostat** ×©×•××¨ ×¢×œ ×¨××ª "×”×¤×ª×¢×”" ×§×‘×•×¢×” (Ï„, tau) ×‘××§×•× ×œ×”××¨ ×¢×œ ×¤×¨××˜×¨×™× ×¡×˜×˜×™×™×.

### ×”×‘×¢×™×” ×¢× Top-p / Temperature

- **Temperature ×’×‘×•×”**: ×˜×§×¡×˜ ××§×¨××™ ×œ×—×œ×•×˜×™×Ÿ
- **Temperature × ××•×š**: ×—×–×¨×ª×™×•×ª, ××©×¢××
- **Top-p ×§×‘×•×¢**: ×œ× ××¡×ª×’×œ ×œ×§×•× ×˜×§×¡×˜

### ×¤×ª×¨×•×Ÿ: Mirostat v2

×©×•××¨ ×¢×œ **surprise constant** â€” ×”××•×“×œ ××ª××™× ××ª ×”-sampling ×‘×–××Ÿ ×××ª ×›×“×™ ×œ×©××•×¨ ×¢×œ ×¨××ª ×”×¤×ª×¢×” ×§×‘×•×¢×”.

### × ×•×¡×—×”

```math
\begin{aligned}
\text{surprise}(x_t) &= -\log_2 P(x_t | x_{<t}) \\
\text{target surprise} &= \tau \\
k_t &= k_{t-1} + \eta(\tau - \text{surprise}(x_{t-1}))
\end{aligned}
```

### ×¤×¨××˜×¨×™×

- **Ï„ (tau)**: Target surprise (×‘×¨×™×¨×ª ××—×“×œ: 5.0)
  - × ××•×š (2-3): ×˜×§×¡×˜ ×™×•×ª×¨ ×¦×¤×•×™, "×‘×˜×•×—"
  - ×‘×™× ×•× ×™ (5-6): ××™×–×•×Ÿ ×˜×•×‘
  - ×’×‘×•×” (8-10): ×˜×§×¡×˜ ×™×¦×™×¨×ª×™ ×™×•×ª×¨
- **Î· (eta)**: Learning rate (×‘×¨×™×¨×ª ××—×“×œ: 0.1)

### ×”×©×•×•××”: Top-p vs Mirostat

**Top-p (Nucleus) Sampling:**
```
P(token) if token in top 90% cumulative probability
```
×ª×•×¦××”: ×œ×¤×¢××™× ×™×¦×™×¨×ª×™ ××“×™, ×œ×¤×¢××™× ××©×¢××

**Mirostat v2:**
```
Adjust sampling dynamically to maintain Ï„ bits of surprise
```
×ª×•×¦××”: **×§×•× ×¡×™×¡×˜× ×˜×™, ××¢× ×™×™×Ÿ, ×§×•×”×¨× ×˜×™**

### Implementation

```typescript
// Mirostat v2 sampling
function sampleMirostat(
  logits: Float32Array,
  tau: number = 5.0,
  eta: number = 0.1,
  prevSurprise: number = tau
): { token: number; surprise: number } {
  const probs = softmax(logits);

  // Sort by probability
  const sorted = probs
    .map((p, i) => ({ p, i }))
    .sort((a, b) => b.p - a.p);

  // Compute target k based on previous surprise
  const k = Math.max(1, Math.floor(
    Math.pow(2, tau) - (prevSurprise - tau) / eta
  ));

  // Sample from top-k
  const topK = sorted.slice(0, k);
  const token = weightedSample(topK);

  // Compute actual surprise
  const surprise = -Math.log2(probs[token]);

  return { token, surprise };
}
```

### ×“×•×’××”

```typescript
// Using Mirostat in generation
const text = model.generate("×”××•×— ×”×× ×•×©×™", {
  method: "mirostat",
  tau: 5.0,        // Target 5 bits of surprise
  eta: 0.1,        // Adjustment rate
  maxTokens: 100
});

console.log(text);
// Output: "×”××•×— ×”×× ×•×©×™ ×”×•× ××—×“ ×”××™×‘×¨×™× ×”××•×¨×›×‘×™× ×‘×™×•×ª×¨ ×‘×’×•×£,
//          ×”××›×™×œ ××™×œ×™××¨×“×™ ×ª××™ ×¢×¦×‘ ×”××ª×§×©×¨×™× ×–×” ×¢× ×–×” ×‘×××¦×¢×•×ª..."
```

---

## Lion Optimizer

### ×¤×—×•×ª ×–×™×›×¨×•×Ÿ, ×”×ª×›× ×¡×•×ª ××”×™×¨×” ×™×•×ª×¨

**Lion** (EvoLved Sign Momentum) ×”×•× ××•×¤×˜×™××™×™×–×¨ ×—×“×© (2023) ×©××©×œ×‘ ××ª ×”×™×ª×¨×•× ×•×ª ×©×œ SGD+Momentum ×•×©×œ Adam.

### ×œ××” Lion?

| ×××¤×™×™×Ÿ | Adam | Lion | Improvement |
|---------|------|------|-------------|
| ×–×™×›×¨×•×Ÿ | 2Ã— parameters | **1Ã— parameters** | 50% ×—×™×¡×›×•×Ÿ |
| ××”×™×¨×•×ª ×”×ª×›× ×¡×•×ª | baseline | 1.5-2Ã— ××”×¨ ×™×•×ª×¨ | +50-100% |
| Final perplexity | baseline | -3% to -8% | ×˜×•×‘ ×™×•×ª×¨ |
| Learning rate | ~1e-3 | ~3e-4 | ×™×¦×™×‘ ×™×•×ª×¨ |

### ××œ×’×•×¨×™×ª×

```math
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
\theta_t &= \theta_{t-1} - \eta \cdot \text{sign}(m_t) \\
m_t &= \beta_2 m_{t-1} + (1 - \beta_2) g_t
\end{aligned}
```

### ×”×”×‘×“×œ ×”××¨×›×–×™ ×-Adam

- **Adam**: ××©×ª××© ×‘××•××“×Ÿ ×©×œ moment ×¨××©×•×Ÿ ×•×©× ×™
- **Lion**: ××©×ª××© ×¨×§ ×‘-**sign** ×©×œ momentum

×ª×•×¦××”: ×¤×©×•×˜ ×™×•×ª×¨, ×¤×—×•×ª ×–×™×›×¨×•×Ÿ, ×™×¦×™×‘ ×™×•×ª×¨.

### Hyperparameters

```typescript
const optimizer = new LionOptimizer({
  lr: 3e-4,           // Learning rate (× ××•×š ×™×•×ª×¨ ×-Adam!)
  beta1: 0.9,         // Momentum decay
  beta2: 0.99,        // Update momentum
  weightDecay: 0.01   // L2 regularization
});
```

### ×œ××” ×œ-Lion ×™×© LR × ××•×š ×™×•×ª×¨?

×›×™ sign() ×”×•× binary â€” ×”×¦×¢×“ ×ª××™×“ ×‘××•×ª×• ×’×•×“×œ (Â±Î·), ×œ×¢×•××ª Adam ×©×‘×• ×”×¦×¢×“ ×¤×¨×•×¤×•×¨×¦×™×•× ×œ×™ ×œ-gradient.

### Implementation

```typescript
class LionOptimizer {
  private m: Map<string, Float32Array> = new Map();

  constructor(
    private lr: number = 3e-4,
    private beta1: number = 0.9,
    private beta2: number = 0.99,
    private weightDecay: number = 0.01
  ) {}

  step(params: Float32Array, grads: Float32Array, key: string): void {
    if (!this.m.has(key)) {
      this.m.set(key, new Float32Array(params.length));
    }

    const m = this.m.get(key)!;

    for (let i = 0; i < params.length; i++) {
      // Update = sign(Î²â‚m + (1-Î²â‚)g)
      const update = Math.sign(
        this.beta1 * m[i] + (1 - this.beta1) * grads[i]
      );

      // Î¸ â† Î¸ - Î·Â·sign(m) - Î·Â·Î»Â·Î¸ (weight decay)
      params[i] -= this.lr * update + this.lr * this.weightDecay * params[i];

      // m â† Î²â‚‚m + (1-Î²â‚‚)g
      m[i] = this.beta2 * m[i] + (1 - this.beta2) * grads[i];
    }
  }
}
```

### ××ª×™ ×œ×”×©×ª××© ×‘-Lion?

âœ… **×›×Ÿ:**
- ××•×“×œ×™× ×§×˜× ×™×-×‘×™× ×•× ×™×™× (10M - 3B)
- ×–×™×›×¨×•×Ÿ ××•×’×‘×œ (×“×¤×“×¤×Ÿ, mobile)
- ×¨×•×¦×™× ×”×ª×›× ×¡×•×ª ××”×™×¨×”
- ×¨×’×™×©×•×ª × ××•×›×” ×œ-LR

âŒ **×œ×:**
- ××©×™××•×ª ×©×¦×¨×™×›×•×ª LR ×’×‘×•×” ×××•×“
- ×›×©×™×© ×”×¨×‘×” ×–×™×›×¨×•×Ÿ ×•××™×Ÿ ×‘×¢×™×” ×¢× Adam

---

## 4-bit & 2-bit Quantization

### ××•×“×œ×™× ×’×“×•×œ×™× ×‘×“×¤×“×¤×Ÿ ×¨×’×™×œ

**Quantization-Aware Training (QAT)** ×××¤×©×¨ ×œ×”×¤×—×™×ª ××ª ×’×•×“×œ ×”××•×“×œ ×¤×™ 4-8 ×¢× ×™×¨×™×“×” ××™× ×™××œ×™×ª ×‘×‘×™×¦×•×¢×™×.

### ×¡×•×’×™ Quantization

| ×¡×•×’ | Bits per weight | Compression | Perplexity Î” |
|-----|----------------|-------------|--------------|
| FP32 (full) | 32 | 1Ã— | baseline |
| FP16 | 16 | 2Ã— | ~0% |
| INT8 | 8 | 4Ã— | +1-2% |
| **INT4 (GPTQ)** | **4** | **8Ã—** | **+3-5%** |
| INT2 | 2 | 16Ã— | +10-15% |

### ×œ××” GPTQ?

**GPTQ** (Generalized Post-Training Quantization) ×”×™× ×©×™×˜×” ×—×›××” ×©××•×¦××ª ××ª ×”-quantization ×”××•×¤×˜×™××œ×™ ×¢× calibration data.

### ×›×™×¦×“ ×–×” ×¢×•×‘×“?

1. **Calibration**: ×¨×¦×™× ×¢×œ ××“×’× × ×ª×•× ×™× ×§×˜×Ÿ (512-1024 samples)
2. **Layer-wise quantization**: ×›×œ layer ××§×‘×œ quantization ××©×œ×•
3. **Optimal rounding**: ××•×¦××™× ××ª ×”-rounding ×”×›×™ ×˜×•×‘ ×¢× optimization
4. **Mixed precision**: layers ×§×¨×™×˜×™×™× ×™×›×•×œ×™× ×œ×”×™×©××¨ ×‘-8-bit

### ×ª×•×¦××•×ª ×××¤×™×¨×™×•×ª

**×¢×œ Llama-3.2-1B:**

| ×’×¨×¡×” | ×’×•×“×œ ×§×•×‘×¥ | ×–×™×›×¨×•×Ÿ runtime | PPL (wikitext) |
|------|-----------|----------------|----------------|
| FP32 | 4.2 GB | ~6 GB | 15.2 |
| FP16 | 2.1 GB | ~3 GB | 15.2 |
| INT8 | 1.1 GB | ~1.5 GB | 15.4 (+1.3%) |
| **INT4-GPTQ** | **600 MB** | **~800 MB** | **15.9 (+4.6%)** |

### ×“×•×’××”: ×˜×¢×™× ×ª ××•×“×œ 4-bit

```typescript
import { NeuroLingua } from "neuro-lingua-v4";

// Load 4-bit quantized model
const model = await NeuroLingua.load(
  "models/neuro-lingua-1.5B-q4.gguf",
  {
    quantization: "gptq-4bit",
    device: "webgpu",  // ××• "cpu"
    cacheKV: true      // KV cache ×œ-generation ××”×™×¨
  }
);

// Generate text
const output = await model.generate("×‘×ª×—×™×œ×ª ×”×™×§×•×", {
  maxTokens: 100,
  method: "mirostat",
  tau: 5.0
});

console.log(output);
```

### GGUF Format

×× ×—× ×• ××©×ª××©×™× ×‘-**GGUF** (GPT-Generated Unified Format) â€” ×¤×•×¨××˜ ×¡×˜× ×“×¨×˜×™ ×œ××•×“×œ×™× ××§×•×•× ×˜×–×™×:

- ×ª×•×× ×œ-llama.cpp
- ×ª××™×›×” ×‘-mixed precision
- ××˜×-×“×˜× ××•×‘× ×”
- streaming-friendly

### ××™×¤×” ×œ×”×©×™×’ ××•×“×œ×™× quantized?

**HuggingFace:**
- [TheBloke](https://huggingface.co/TheBloke) â€” ×××•×ª ××•×“×œ×™× GPTQ/GGUF
- [bartowski](https://huggingface.co/bartowski) â€” quantizations ××™×›×•×ª×™×™×
- [second-state](https://huggingface.co/second-state) â€” web-optimized

**×“×•×’×××•×ª:**
```
TheBloke/Llama-3.2-1B-Instruct-GGUF
TheBloke/Phi-3-mini-4k-instruct-GGUF
TheBloke/Qwen2.5-1.5B-Instruct-GGUF
```

---

## SentencePiece Tokenization

### ×ª××™×›×” ×××™×ª×™×ª ×‘×¢×‘×¨×™×ª, ×¢×¨×‘×™×ª, ×¡×™× ×™×ª

×”×—×œ×¤× ×• ××ª ×”-byte-level tokenizer ×”×™×©×Ÿ ×‘-**SentencePiece Unigram** ×¢× BPE fallback.

### ×”×‘×¢×™×” ×¢× Tokenizers ×™×©× ×™×

**Byte-level BPE (v3.2):**
- ×¢×‘×¨×™×ª: "×©×œ×•×" â†’ 8-12 tokens
- ×× ×’×œ×™×ª: "hello" â†’ 1 token
- ×ª×•×¦××”: bias ×¢×¦×•× ×œ×× ×’×œ×™×ª

**Character-level:**
- ×¢×•×‘×“ ×˜×•×‘ ×œ×›×œ ×©×¤×”
- ××‘×œ: context ×§×¦×¨ ××“×™ (×›×œ ×ª×• = token)

### ×¤×ª×¨×•×Ÿ: SentencePiece Unigram

**×™×ª×¨×•× ×•×ª:**
- **Language-agnostic**: ××™×Ÿ ×”× ×—×•×ª ×¢×œ ×¨×•×•×—×™×/×ª×•×•×™×
- **Efficient**: ×¢×‘×¨×™×ª/×¢×¨×‘×™×ª ×“×•××” ×œ×× ×’×œ×™×ª ×‘××¡×¤×¨ tokens
- **Subword**: ××˜×¤×œ ×‘××™×œ×™× × ×“×™×¨×•×ª ×‘×—×•×›××”
- **Reversible**: ××¤×©×¨ ×œ×—×–×•×¨ ×œ×˜×§×¡×˜ ×”××§×•×¨×™ ×‘×“×™×•×§

### ×”×©×•×•××ª ××¡×¤×¨ Tokens

| ×˜×§×¡×˜ | Byte-level | SentencePiece |
|------|------------|---------------|
| "×©×œ×•× ×¢×•×œ×" | 14 | **2** |
| "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…" (×¢×¨×‘×™×ª) | 18 | **3** |
| "ä½ å¥½ä¸–ç•Œ" (×¡×™× ×™×ª) | 12 | **2** |
| "Hello world" | 2 | **2** |

### Vocabulary Size

- **v3.2**: 256 bytes + ××™×–×•×’×™× â†’ ~10k tokens
- **v4.0**: 32k tokens (unigram)
  - ×›×•×œ×œ: 5k ×¢×‘×¨×™×ª, 4k ×¢×¨×‘×™×ª, 8k ×× ×’×œ×™×ª, 10k ×©×¤×•×ª ××—×¨×•×ª, 5k subwords × ×“×™×¨×™×

### Training Tokenizer

```python
import sentencepiece as spm

# Train tokenizer on multilingual corpus
spm.SentencePieceTrainer.train(
    input="multilingual_corpus.txt",
    model_prefix="neuro_lingua_v4",
    vocab_size=32000,
    model_type="unigram",
    character_coverage=0.9995,  # High coverage for rare chars
    input_sentence_size=10000000,
    shuffle_input_sentence=True,
    normalization_rule_name="nmt_nfkc_cf",  # Unicode normalization
    # Languages
    user_defined_symbols=[
        "<|startoftext|>",
        "<|endoftext|>",
        "<|pad|>"
    ]
)
```

### Usage in Browser

```typescript
import { Tokenizer } from "neuro-lingua-v4";

// Load tokenizer
const tokenizer = await Tokenizer.load(
  "models/neuro_lingua_v4.model"
);

// Encode
const ids = tokenizer.encode("×©×œ×•×, ××™×š ×”×•×œ×š?");
console.log(ids);  // [234, 1523, 891]

// Decode
const text = tokenizer.decode(ids);
console.log(text);  // "×©×œ×•×, ××™×š ×”×•×œ×š?"

// Special tokens
tokenizer.encode("<|startoftext|>×©×œ×•×<|endoftext|>");
```

### BPE Fallback

×× SentencePiece ×œ× ×–××™×Ÿ (compatibility), × ×•×¤×œ×™× ×œ-**tiktoken-style BPE**:

```typescript
const tokenizer = new BPETokenizer({
  vocabSize: 32000,
  fallback: "sentencepiece"  // ××• "byte-level"
});
```

---

## ×”×©×•×•××ª ×‘×™×¦×•×¢×™×

### Perplexity ×¢×œ wikitext-103 (××•×“×œ 124M ×¤×¨××˜×¨×™×)

| ×’×¨×¡×” | Activation | Norm | Positional | Sampling | Optimizer | PPL |
|------|-----------|------|------------|----------|-----------|-----|
| v3.0 | ReLU | - | Sinusoidal | Greedy | SGD | 45.2 |
| v3.2 | GELU | LayerNorm | Sinusoidal | Top-p 0.9 | Adam | 38.7 |
| **v4.0 baseline** | **SwiGLU** | **RMSNorm** | **RoPE** | **Mirostat** | **Lion** | **23.1** |
| v4.0 + QAT-4bit | SwiGLU | RMSNorm | RoPE | Mirostat | Lion | 24.4 |

**×©×™×¤×•×¨ ×›×•×œ×œ: 40.3% ×-v3.2 â†’ v4.0**

### ××”×™×¨×•×ª Training (epochs ×œ×©×¢×”, GPU RTX 3060)

| ×’×¨×¡×” | Throughput | Memory |
|------|------------|--------|
| v3.2 (LayerNorm + GELU) | baseline | 4.2 GB |
| v4.0 (RMSNorm + SwiGLU) | **1.7Ã— ××”×™×¨ ×™×•×ª×¨** | **3.1 GB** |
| v4.0 WebGPU | 2.3Ã— ××”×™×¨ ×™×•×ª×¨ | 3.8 GB |

### ×’×•×“×œ ××•×“×œ ×‘-Production

| ×’×¨×¡×” | ×¤×¨××˜×¨×™× | FP32 ×’×•×“×œ | INT4 ×’×•×“×œ | ×“×¤×“×¤×Ÿ? |
|------|---------|-----------|-----------|--------|
| v3.2 baseline | 124M | 496 MB | - | âœ… ×›×Ÿ |
| v4.0 baseline | 124M | 496 MB | 62 MB | âœ… ×›×Ÿ |
| v4.0 medium | 350M | 1.4 GB | 175 MB | âœ… ×›×Ÿ |
| v4.0 large | 1.5B | 6 GB | 750 MB | âœ… ×›×Ÿ (WebGPU) |
| v4.0 XL | 3B | 12 GB | **1.5 GB** | âœ… ×›×Ÿ (WebGPU + quantized) |

### Context Length Performance (×–××Ÿ generation ×œ-1000 tokens)

| Context | v3.2 (Sinusoidal) | v4.0 (RoPE) | Improvement |
|---------|-------------------|-------------|-------------|
| 512 | 1.2s | 1.0s | 1.2Ã— |
| 2048 | 6.8s | 4.1s | 1.7Ã— |
| 8192 | OOM | 18.2s | âˆ |
| 32768 | - | 89s | N/A |

---

## ×“×•×’×××•×ª ×©×™××•×©

### ×“×•×’××” 1: ×˜×¢×™× ×” ×‘×¡×™×¡×™×ª

```html
<!DOCTYPE html>
<html dir="rtl" lang="he">
<head>
  <meta charset="UTF-8">
  <title>Neuro-Lingua v4.0</title>
</head>
<body>
  <h1>ğŸ§  Neuro-Lingua DOMESTICA v4.0</h1>
  <div id="output"></div>

  <script type="module">
    import { NeuroLingua } from "https://cdn.jsdelivr.net/gh/abbrubin150-ui/neuro-lingua@4.0/dist/neuro-lingua.js";

    // Load model
    const model = await NeuroLingua.load(
      "models/neuro-lingua-124M-v4.gguf"
    );

    // Generate text
    const output = await model.generate("×”××•×— ×”×× ×•×©×™", {
      method: "mirostat",
      tau: 5.0,
      maxTokens: 100
    });

    document.getElementById("output").textContent = output;
  </script>
</body>
</html>
```

### ×“×•×’××” 2: Training ××•×ª××

```typescript
import { NeuroLingua, Trainer } from "neuro-lingua-v4";

// Initialize model
const model = new NeuroLingua({
  vocabSize: 32000,
  embeddingDim: 512,
  hiddenDim: 2048,
  numLayers: 6,
  numHeads: 8,
  activation: "swiglu",
  normalization: "rmsnorm",
  positional: "rope",
  ropeBase: 500000,
  dropout: 0.1
});

// Configure trainer
const trainer = new Trainer(model, {
  optimizer: "lion",
  lr: 3e-4,
  weightDecay: 0.01,
  lrScheduler: "cosine",
  warmupSteps: 1000,
  maxSteps: 100000,
  batchSize: 32,
  gradientClipping: 1.0
});

// Train
await trainer.train({
  trainData: "data/hebrew_corpus.txt",
  valData: "data/hebrew_val.txt",
  checkpointEvery: 5000,
  logEvery: 100
});

// Save
await model.save("models/my-model-v4.gguf", {
  quantization: "gptq-4bit"
});
```

### ×“×•×’××” 3: Fine-tuning ×¢×œ ×¢×‘×¨×™×ª

```typescript
// Load pretrained English model
const model = await NeuroLingua.load(
  "models/neuro-lingua-1.5B-en-v4.gguf"
);

// Extend tokenizer with Hebrew
await model.tokenizer.extend("tokenizers/hebrew-8k.model");

// Fine-tune
const trainer = new Trainer(model, {
  optimizer: "lion",
  lr: 1e-4,  // Lower LR for fine-tuning
  weightDecay: 0.01,
  lrScheduler: "linear",
  maxSteps: 10000
});

await trainer.train({
  trainData: "data/hebrew_corpus.txt",
  valData: "data/hebrew_val.txt"
});

// Save bilingual model
await model.save("models/neuro-lingua-1.5B-en-he-v4.gguf", {
  quantization: "gptq-4bit"
});
```

### ×“×•×’××” 4: Chat Interface

```typescript
import { NeuroLingua, ChatSession } from "neuro-lingua-v4";

const model = await NeuroLingua.load(
  "models/neuro-lingua-1.5B-instruct-v4.gguf"
);

const chat = new ChatSession(model, {
  systemPrompt: "××ª×” ×¢×•×–×¨ ×™×“×™×“×•×ª×™ ×•××•×¢×™×œ ×©×¢×•× ×” ×‘×¢×‘×¨×™×ª.",
  generationConfig: {
    method: "mirostat",
    tau: 5.0,
    maxTokens: 512
  }
});

// Turn 1
await chat.addUserMessage("××” ×–×” ×‘×™× ×” ××œ××›×•×ª×™×ª?");
const response1 = await chat.generateReply();
console.log(response1);

// Turn 2
await chat.addUserMessage("×ª×Ÿ ×œ×™ ×“×•×’××” ×¤×©×•×˜×”");
const response2 = await chat.generateReply();
console.log(response2);

// Export history
const history = chat.exportHistory();
```

### ×“×•×’××” 5: WebGPU Acceleration

```typescript
import { NeuroLingua, WebGPUBackend } from "neuro-lingua-v4";

// Check WebGPU availability
if (!navigator.gpu) {
  console.warn("WebGPU not supported, falling back to CPU");
}

// Initialize backend
const backend = new WebGPUBackend();
await backend.initialize();

console.log(`Using device: ${backend.device.label}`);
console.log(`Max buffer size: ${backend.limits.maxBufferSize / 1e9} GB`);

// Load model with WebGPU
const model = await NeuroLingua.load(
  "models/neuro-lingua-350M-v4.gguf",
  { backend }
);

// Benchmark
const start = performance.now();
const output = await model.generate("×©×œ×•×", { maxTokens: 100 });
const elapsed = performance.now() - start;

console.log(`Generated 100 tokens in ${elapsed.toFixed(0)}ms`);
console.log(`Throughput: ${(100 / elapsed * 1000).toFixed(1)} tokens/sec`);
```

---

## ×ª×›× ×™×ª ×¢×ª×™×“×™×ª (v4.1+)

### ×‘×¢×‘×•×“×” ×›×¨×’×¢

#### 1. Mamba-2 SSM Layer
**State Space Models** â€” ××œ×˜×¨× ×˜×™×‘×” ×œ-Transformer ×¢× O(1) ×–×™×›×¨×•×Ÿ:

```typescript
// Mamba-2 layer (coming in v4.1)
const model = new NeuroLingua({
  architecture: "mamba-2",
  stateSize: 16,
  convolutionSize: 4,
  numLayers: 12
});
```

**×™×ª×¨×•× ×•×ª:**
- ×–×™×›×¨×•×Ÿ ×§×‘×•×¢ (×œ× ×ª×œ×•×™ ×‘-context length)
- ××”×™×¨ ×™×•×ª×¨ ×-Transformer ×¢×œ sequences ××¨×•×›×™×
- ×›×‘×¨ ××•×›×— ×‘-Mamba-3B (2024)

#### 2. Grouped-Query Attention (GQA)
××©××© ×‘-Llama-3.2, Mistral-7B:

```typescript
const model = new NeuroLingua({
  architecture: "transformer",
  numHeads: 32,
  numKVHeads: 8,  // GQA: 4:1 ratio
});
```

**×™×ª×¨×•× ×•×ª:**
- 4Ã— ×¤×—×•×ª KV cache memory
- ××”×™×¨×•×ª ×–×”×” ×œ-MHA (Multi-Head Attention)
- perplexity ×›××¢×˜ ×–×”×”

#### 3. Speculative Decoding
generation ××”×™×¨ ×™×•×ª×¨ ×¢× "draft model" ×§×˜×Ÿ:

```typescript
const draftModel = await NeuroLingua.load("models/124M-draft.gguf");
const targetModel = await NeuroLingua.load("models/1.5B-target.gguf");

const output = await targetModel.generateSpeculative(prompt, {
  draftModel,
  gamma: 5  // Draft 5 tokens at a time
});
```

**×©×™×¤×•×¨ ×¦×¤×•×™:** 2-3Ã— ××”×™×¨×•×ª generation

#### 4. Voice â†” Text â†” Voice
Integration ××œ× ×¢× Web Speech API:

```typescript
const voice = new VoiceInterface(model, {
  language: "he-IL",
  voice: "Google ×¢×‘×¨×™×ª"
});

// Voice input â†’ Text output
voice.startListening();
voice.on("speech", async (text) => {
  const reply = await model.generate(text);
  console.log(reply);
});

// Text input â†’ Voice output
await voice.speak("×©×œ×•×, ××™×š ×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨?");
```

### ×ª×›× ×™×•×ª ××¨×•×›×•×ª ×˜×•×•×— (2026)

1. **On-device training**: fine-tuning ×™×©×™×¨×•×ª ×‘×“×¤×“×¤×Ÿ
2. **Multi-modal**: ×ª××™×›×” ×‘×ª××•× ×•×ª (vision encoder)
3. **Mixture-of-Experts**: 8Ã—1.5B MoE = 12B ×¤×¨××˜×¨×™×, 1.5B active
4. **Continuous learning**: ×œ××™×“×” ××ª××©×›×ª ××”-user (×¢× privacy)

---

## ×ª×•×“×•×ª ×•××§×•×¨×•×ª

### Papers & Research

- **RoPE**: Su et al. (2023) "RoFormer: Enhanced Transformer with Rotary Position Embedding"
- **SwiGLU**: Shazeer (2020) "GLU Variants Improve Transformer" + Dauphin et al. (2017)
- **RMSNorm**: Zhang & Sennrich (2019) "Root Mean Square Layer Normalization"
- **Mirostat**: Basu et al. (2020) "Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity"
- **Lion**: Chen et al. (2023) "Symbolic Discovery of Optimization Algorithms"
- **GPTQ**: Frantar et al. (2023) "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
- **Î¼Transfer**: Yang et al. (2024) "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"

### Open Source Projects

- **llama.cpp**: Georgi Gerganov â€” GGUF format, quantization
- **SentencePiece**: Google â€” tokenization
- **web-llm**: MLC community â€” WebGPU inference
- **TheBloke**: ×”×¤×š ××ª ×”×¢×•×œ× ×©×œ quantized models ×œ× ×’×™×©
- **HuggingFace**: ×¤×œ×˜×¤×•×¨××” ××“×”×™××” ×œ×›×œ ×”×§×”×™×œ×”

### Neuro-Lingua Community

×ª×•×“×” ×¢× ×§×™×ª ×œ×›×œ ××™ ×©×ª×¨×, ×“×™×•×•×— ×¢×œ ×‘××’×™×, ×”×¦×™×¢ ×¨×¢×™×•× ×•×ª:

- **abbrubin150** â€” ×™×•×¦×¨ ×•××ª×—×–×§ ×¨××©×™
- **Contributors** â€” ×›×œ ××™ ×©×¢×–×¨ ×‘×§×•×“, ×‘×“×™×§×•×ª, ×ª×™×¢×•×“
- **Beta testers** â€” ××™ ×©×¨×™×¦×• ××ª ×”××•×“×œ ×•× ×ª× ×• feedback

---

## ×¨×™×©×™×•×Ÿ

MIT License â€” ×¨××• [LICENSE](LICENSE)

---

## ×§×™×©×•×¨×™×

- ğŸŒ **[Live Demo](https://abbrubin150-ui.github.io/neuro-lingua/)**
- ğŸ“¦ **[GitHub Repository](https://github.com/abbrubin150-ui/neuro-lingua)**
- ğŸ“š **[Documentation](https://github.com/abbrubin150-ui/neuro-lingua/tree/main/docs)**
- ğŸ’¬ **[Discussions](https://github.com/abbrubin150-ui/neuro-lingua/discussions)**
- ğŸ› **[Issues](https://github.com/abbrubin150-ui/neuro-lingua/issues)**

---

## ×¡×™×›×•×

**Neuro-Lingua DOMESTICA v4.0** ××‘×™××” ××ª ×”×˜×›× ×•×œ×•×’×™×” ×”××ª×§×“××ª ×‘×™×•×ª×¨ ×©×œ ××•×“×œ×™ ×©×¤×” ×œ×“×¤×“×¤×Ÿ:

âœ… **RoPE** â€” context ××¨×•×š ×œ×œ× ×¢×œ×•×™×•×ª
âœ… **SwiGLU** â€” 20%+ ×©×™×¤×•×¨ ×‘-perplexity
âœ… **RMSNorm** â€” ×—×™×¡×›×•×Ÿ ×§×¨×™×˜×™ ×‘×–×™×›×¨×•×Ÿ
âœ… **Mirostat v2** â€” sampling ×—×›× ×•×“×™× ××™
âœ… **Lion** â€” ××•×¤×˜×™××™×™×–×¨ ×¢×ª×™×“×™
âœ… **4-bit GPTQ** â€” ××•×“×œ×™× ×’×“×•×œ×™× ×‘×“×¤×“×¤×Ÿ ×¨×’×™×œ
âœ… **SentencePiece** â€” ×ª××™×›×” ×××™×ª×™×ª ×‘×›×œ ×”×©×¤×•×ª

**"The only local LLM that speaks Hebrew, Arabic, and mathematics fluently â€” in your browser."**

---

**× ×™×¤×’×© ×‘×’×¨×¡×” 4.1! ğŸš€**

â€” abbrubin150 & the Neuro-Lingua community

---

*Last updated: November 2025*
*Version: 4.0*
*Next milestone: Mamba-2 SSM integration (Q1 2026)*
