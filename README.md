# Neuroâ€‘Lingua DOMESTICA â€” v3.2.4 (EN)

**Browserâ€‘native neural language model** built in React + TypeScript.

ğŸŒ **[Try the live demo â†’](https://abbrubin150-ui.github.io/neuro-lingua/)** â€” includes an English â†” Hebrew toggle for the UI

## Core Features

- **Multiple Architectures**: Standard ProNeuralLM, AdvancedNeuralLM, and fully-functional Transformer models with multi-head attention
- **WebGPU Acceleration**: 2-5x faster training on compatible hardware with automatic CPU fallback
- SGD with **Momentum**, **Adam**, **Damped Newton**, or **L-BFGS** optimization
- **Dropout** (trainâ€‘only)
- **Advanced Text Generation**: Greedy, Sampling (Top-p/Top-k), Beam Search, and Contrastive decoding
- **Session persistence**, onboarding tips, and downloadable **training-history CSVs** (localized labels)
- **Tokenizer presets** (Unicode/ASCII/custom) with import/export support
- **Agent** workflow: a single GitHub Action retrains the model and commits the updated JSON artifact

## ğŸš€ Advanced Features

### Neural Network Architectures

- **ğŸ”® Transformer**: Fully-implemented multi-head self-attention with position embeddings, residual connections, and layer normalization (configurable layers and heads)
- **ğŸš€ AdvancedNeuralLM**: State-of-the-art feedforward architecture
- **ğŸ“Š ProNeuralLM**: Standard baseline model

### Mathematical Enhancements

- âœ… **He/Xavier Initialization** - Faster convergence with proper weight init
- âœ… **Advanced Activations** - LeakyReLU, ELU, GELU, Swish
- âœ… **Learning Rate Scheduling** - Cosine annealing, exponential decay, warmup
- âœ… **L2 Regularization** - Weight decay for better generalization
- âœ… **Layer Normalization** - Training stability
- âœ… **Numerical Stability** - Log-sum-exp, stable softmax
- âœ… **Perplexity Calculation** - Model evaluation metric

### Text Generation Methods

- âœ… **Greedy Decoding** - Deterministic selection of most likely token (argmax)
- âœ… **Temperature Sampling** - Controlled randomness in generation
- âœ… **Top-k Sampling** - Sample from k most likely tokens
- âœ… **Nucleus (Top-p) Sampling** - Sample from smallest set with cumulative probability p
- âœ… **Beam Search** - Maintain multiple hypotheses for higher quality output
- âœ… **Contrastive Search** - Balance model confidence with diversity to reduce repetition

### Performance Optimization

- âœ… **WebGPU Acceleration** - Hardware-accelerated training with GPU
- âœ… **GPU Metrics Dashboard** - Real-time performance monitoring
- âœ… **Automatic Fallback** - Seamless CPU fallback when GPU unavailable

### ğŸ¯ Experiment Management (Î£-SIG Compliance)

- âœ… **Project & Run Architecture** - Full experiment tracking with frozen configurations
- âœ… **Decision Ledger** - Governance framework with rationale, witness, and expiry tracking
- âœ… **Scenario Testing** - Define and evaluate test scenarios across multiple runs
- âœ… **Execution Status** - EXECUTE/HOLD/ESCALATE compliance checks
- âœ… **Trace Export** - Complete audit trail with model weights, configs, and metadata

### ğŸ” Model Interpretability

- âœ… **SHAP Values** - Estimate feature importance using Shapley values
- âœ… **Integrated Gradients** - Attribution method for input token contributions
- âœ… **Attention Rollout** - Visualize attention flow in Transformer models
- âœ… **Explainability Panel** - Interactive UI for model interpretation

### ğŸ“Š Advanced Visualization

- âœ… **Embedding Visualization** - Interactive t-SNE and UMAP projections
- âœ… **Information Theory Panel** - I(X;Z) vs I(Z;Y) information plane
- âœ… **Information Bottleneck** - Compression-prediction trade-off analysis
- âœ… **Canvas Interaction** - Pan, zoom, and explore embedding spaces

ğŸ“š **[See full mathematical documentation â†’](./MATHEMATICAL_ENHANCEMENTS.md)**
ğŸ“– **[Transformer architecture guide â†’](./TRANSFORMER_GUIDE.md)**
âš¡ **[GPU acceleration setup â†’](./GPU_ACCELERATION_GUIDE.md)**

> This repository is intentionally simple: the only thing your agent does is **train** and **update** the model JSON.
> The tokenizer is languageâ€‘agnostic, while the UI strings are available in English and Hebrew.

### Localization & translations

- Use the language toggle in the navbar to switch between English (LTR) and Hebrew (RTL) for the training editor, onboarding card, info cards, and chat console labels.
- Advanced configuration options, chart axes, and logs are currently English-only. Contributions that extend localization to those areas are welcome.
- Translations live directly in [`src/App.tsx`](./src/App.tsx) inside the `TRANSLATIONS` map so you can add new locales without touching other components.

---

## Quickstart

```bash
# 1) Install
pnpm i  # or: npm i / yarn

# 2) Dev
pnpm dev

# 3) Quality
pnpm lint && pnpm test

# 4) Build
pnpm build && pnpm preview

# 5) Train (Node script, no browser needed)
pnpm train

# 6) GPU Benchmarks (optional)
pnpm benchmark:gpu
```

The browser UI allows you to paste a training corpus and interact with the model.  
The Node training script (`scripts/train.ts`) reads from `data/corpus.txt` and writes the artifact to `models/neuroâ€‘linguaâ€‘v324.json`.

---

## Repo layout

```
.
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ ci.yml                      # Continuous integration (test, lint, build)
â”‚   â”œâ”€â”€ train-model.yml             # Automated model retraining
â”‚   â””â”€â”€ deploy-pages.yml            # GitHub Pages deployment
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus.txt                  # training corpus for the agent
â”‚   â”œâ”€â”€ raw/                        # raw datasets (wikitext, hebrew_news)
â”‚   â””â”€â”€ processed/                  # preprocessed train/val/test splits
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ experiments/                # experiment results and summaries
â”‚   â”œâ”€â”€ theory/                     # theoretical documentation
â”‚   â””â”€â”€ visuals/                    # embedding visualization exports
â”œâ”€â”€ models/
â”‚   â””â”€â”€ neuro-lingua-v324.json      # latest trained model artifact (3MB)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.ts                    # Node training script (ts-node)
â”‚   â”œâ”€â”€ benchmark_gpu.ts            # GPU performance benchmarks
â”‚   â””â”€â”€ visualize_embeddings.ts     # Generate t-SNE/UMAP visualizations
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ webgpu.ts               # WebGPU backend and tensor operations
â”‚   â”‚   â””â”€â”€ gpu_neural_ops.ts      # High-level neural operations on GPU
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ TrainingPanel.tsx       # Main training configuration panel
â”‚   â”‚   â”œâ”€â”€ ModelMetrics.tsx        # Performance metrics dashboard
â”‚   â”‚   â”œâ”€â”€ ProjectManager.tsx      # Project/run management (Î£-SIG)
â”‚   â”‚   â”œâ”€â”€ ScenarioManager.tsx     # Test scenario editor
â”‚   â”‚   â”œâ”€â”€ DecisionLedgerEditor.tsx # Governance/decision tracking
â”‚   â”‚   â”œâ”€â”€ ExplainabilityPanel.tsx # SHAP/gradients/attention visualization
â”‚   â”‚   â”œâ”€â”€ EmbeddingVisualizationPanel.tsx # t-SNE/UMAP interactive canvas
â”‚   â”‚   â”œâ”€â”€ InformationTheoryPanel.tsx # Information bottleneck metrics
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx       # Chat-style generation UI
â”‚   â”‚   â””â”€â”€ TokenizerConfig.tsx     # Tokenizer settings
â”‚   â”œâ”€â”€ contexts/
â”‚   â”‚   â””â”€â”€ ProjectContext.tsx      # Project/run state management
â”‚   â”œâ”€â”€ explainability/
â”‚   â”‚   â”œâ”€â”€ shap.ts                 # SHAP values
â”‚   â”‚   â”œâ”€â”€ integrated_gradients.ts # Integrated gradients
â”‚   â”‚   â””â”€â”€ attention_rollout.ts    # Attention visualization
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ sampler.ts              # Top-k, top-p, temperature sampling
â”‚   â”‚   â”œâ”€â”€ beam_search.ts          # Beam search implementation
â”‚   â”‚   â””â”€â”€ contrastive_search.ts   # Contrastive decoding
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ ProNeuralLM.ts          # Base feedforward LM
â”‚   â”‚   â”œâ”€â”€ AdvancedNeuralLM.ts     # Enhanced LM with advanced features
â”‚   â”‚   â”œâ”€â”€ TransformerLM.ts        # Transformer architecture
â”‚   â”‚   â”œâ”€â”€ MathUtils.ts            # Numerical stability utilities
â”‚   â”‚   â”œâ”€â”€ storage.ts              # localStorage abstraction
â”‚   â”‚   â”œâ”€â”€ utils.ts                # Tokenizer and CSV utilities
â”‚   â”‚   â””â”€â”€ traceExport.ts          # Î£-SIG compliant experiment tracing
â”‚   â”œâ”€â”€ losses/                     # Advanced loss functions
â”‚   â”œâ”€â”€ training/                   # Optimization algorithms
â”‚   â”œâ”€â”€ types/                      # TypeScript type definitions
â”‚   â”œâ”€â”€ visualization/              # Embedding visualization (t-SNE, UMAP)
â”‚   â”œâ”€â”€ App.tsx                     # Main React application
â”‚   â””â”€â”€ main.tsx                    # Application entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ ProNeuralLM.test.ts         # Core model tests
â”‚   â”œâ”€â”€ AdvancedNeuralLM.test.ts    # Advanced features tests
â”‚   â”œâ”€â”€ TransformerLM.test.ts       # Transformer tests
â”‚   â””â”€â”€ numerics/                   # Numerical correctness tests
â”œâ”€â”€ index.html
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ vite.config.ts
â”œâ”€â”€ CLAUDE.md                       # AI assistant development guide
â”œâ”€â”€ MATHEMATICAL_ENHANCEMENTS.md    # Detailed math formulations
â”œâ”€â”€ TRANSFORMER_GUIDE.md            # Transformer architecture explanation
â”œâ”€â”€ GPU_ACCELERATION_GUIDE.md       # WebGPU setup and usage
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## Agent / CI

- **Workflow**: `.github/workflows/train-model.yml`
- **Triggers**:
  - Manual `workflow_dispatch` inputs for `epochs`, `optimizer`, `dropout`
  - `push` events that touch `data/corpus.txt` or `scripts/train.ts`
- **Steps**:
  1. Checkout the repo with history so model diffs can be detected.
  2. Install dependencies via `pnpm install --frozen-lockfile`.
  3. Run `pnpm train` with those inputs provided as `EPOCHS`, `OPTIMIZER`, `DROPOUT` environment variables.
  4. Commit and push `models/neuro-lingua-v324.json` if the artifact changed.

Because the workflow pushes commits, the token it runs with must have `contents: write` access:

- Repository â†’ Settings â†’ Actions â†’ General â†’ Workflow permissions â†’ **Read and write permissions**
- For forks, supply a PAT (for example `secrets.WORKFLOW_TOKEN`) and configure the job to use it before `git push`.

Example manual dispatch:

```bash
gh workflow run train-model.yml \
  -f epochs=40 \
  -f optimizer=adam \
  -f dropout=0.15
```

---

## âš ï¸ Important Warnings

### Privacy & Sensitive Data

**DO NOT use this application with:**

- Personally Identifiable Information (PII)
- Sensitive personal data
- Confidential business information
- Medical records or health data
- Financial information
- Authentication credentials or secrets

**Why?** This application stores training data and models in browser localStorage, which:

- Is not encrypted
- Persists across sessions
- Could be accessed by browser extensions or malicious scripts
- May be included in browser sync/backup

**Recommendation:** Use only public, non-sensitive text for training and experimentation.

---

## ğŸ¯ Experiment Management with Î£-SIG Compliance

Neuro-Lingua implements the **Î£-SIG (Scientific Infrastructure for Governance)** framework for reproducible experiment tracking:

### Projects & Runs

- **Projects** organize related training experiments with shared goals
- **Runs** capture frozen training configurations with complete snapshots:
  - All hyperparameters (frozen and immutable after creation)
  - Architecture configuration (ProNeuralLM/AdvancedNeuralLM/TransformerLM)
  - Tokenizer settings
  - Training corpus with checksum
  - Complete training history and results
  - Serialized model weights

### Decision Ledger

Every run includes a governance layer with:

- **Rationale**: Why this training run is necessary
- **Witness**: Who authorized the training (e.g., "local-user")
- **Expiry**: Optional expiration date (ISO 8601)
- **Rollback**: Action after expiry (keep/delete-after-expiry/archive)
- **Execution Status**: EXECUTE âœ… / HOLD â¸ï¸ / ESCALATE ğŸš¨

### Scenario Testing

Define test scenarios per project:

- **Prompt**: Input text for generation
- **Expected Response**: Optional reference output
- **Scoring**: Track performance across multiple runs
- **Comparison**: Evaluate which configuration performs best

### Trace Export

Export models with complete audit trail including project metadata, decision ledger, training trace, and full reproducibility information. See `CHANGELOG_v3.3.md` for the complete export format specification.

---

## ğŸ” Model Interpretability & Visualization

### Explainability Methods

- **SHAP Values**: Estimate token importance using Shapley value approximation
- **Integrated Gradients**: Attribution method measuring token contributions
- **Attention Rollout**: Visualize attention flow in Transformer models (multi-head support)

### Embedding Visualization

- **t-SNE Projection**: Interactive 2D visualization of token embeddings
- **UMAP Projection**: Alternative dimensionality reduction with configurable parameters
- **Canvas Interaction**: Pan, zoom, and explore embedding spaces
- **Export**: Save visualizations for documentation

### Information Theory

- **Information Bottleneck**: I(X;Z) vs I(Z;Y) information plane visualization
- **Compression-Prediction Trade-off**: Balance between model compression and prediction accuracy
- **Entropy Metrics**: H(Z), H(Z|X) tracking during training

---

## ğŸ“š Documentation

- **[CLAUDE.md](./CLAUDE.md)** - Comprehensive AI assistant development guide
- **[MATHEMATICAL_ENHANCEMENTS.md](./MATHEMATICAL_ENHANCEMENTS.md)** - Detailed mathematical formulations
- **[TRANSFORMER_GUIDE.md](./TRANSFORMER_GUIDE.md)** - Transformer architecture deep dive
- **[TRANSFORMER_IMPLEMENTATION.md](./TRANSFORMER_IMPLEMENTATION.md)** - Implementation details
- **[GPU_ACCELERATION_GUIDE.md](./GPU_ACCELERATION_GUIDE.md)** - WebGPU setup and benchmarking
- **[DEVELOPMENT_SETUP_GUIDE.md](./DEVELOPMENT_SETUP_GUIDE.md)** - Development environment setup
- **[CHANGELOG_v3.3.md](./CHANGELOG_v3.3.md)** - Project & Run management release notes

---

## Notes

- The LM is educational and runs fully in the browser. It is **not** optimized for long texts.
- Browser sessions persist hyperparameters, tokenizer configuration, and training corpora via `localStorage`. Use the onboarding card in the UI to review import/export and pause/resume behaviour.
- Download the training-history CSV from the statistics panel to compare runs.
- The tokenizer uses a Unicode-aware rule by default. Override via environment when training headlessly: set `TOKENIZER_MODE=ascii` or provide `TOKENIZER_MODE=custom` with `TOKENIZER_PATTERN="[^a-z]+"`. The UI exposes the same presets and allows exporting/importing tokenizer JSON files.
- The Node training script now adds `<PAD>` to the vocabulary to match the browser experience.

Enjoy!
