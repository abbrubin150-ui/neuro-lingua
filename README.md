# Neuroâ€‘Lingua DOMESTICA â€” v3.2.4 (EN)

**Browserâ€‘native neural language model** built in React + TypeScript.

ğŸŒ **[Try the live demo â†’](https://abbrubin150-ui.github.io/neuro-lingua/)**

## Core Features

- **Multiple Architectures**: Standard ProNeuralLM, AdvancedNeuralLM, and Transformer models
- **WebGPU Acceleration**: 2-5x faster training on compatible hardware with automatic CPU fallback
- SGD with **Momentum**, **Adam**, **Damped Newton**, or **L-BFGS** optimization
- **Dropout** (trainâ€‘only)
- **Topâ€‘p** (nucleus) and **Topâ€‘k** sampling with temperature
- **Session persistence**, onboarding tips, and downloadable **training-history CSVs**
- **Tokenizer presets** (Unicode/ASCII/custom) with import/export support
- **Agent** workflow: a single GitHub Action retrains the model and commits the updated JSON artifact

## ğŸš€ Advanced Features

### Neural Network Architectures

- **ğŸ”® Transformer**: Multi-head self-attention with position embeddings (2 layers, 4 heads)
- **ğŸš€ AdvancedNeuralLM**: State-of-the-art feedforward architecture
- **ğŸ“Š ProNeuralLM**: Standard baseline model

### Mathematical Enhancements

- âœ… **He/Xavier Initialization** - Faster convergence with proper weight init
- âœ… **Advanced Activations** - LeakyReLU, ELU, GELU, Swish
- âœ… **Learning Rate Scheduling** - Cosine annealing, exponential decay, warmup
- âœ… **L2 Regularization** - Weight decay for better generalization
- âœ… **Layer Normalization** - Training stability
- âœ… **Beam Search** - Higher quality text generation
- âœ… **Numerical Stability** - Log-sum-exp, stable softmax
- âœ… **Perplexity Calculation** - Model evaluation metric

### Performance Optimization

- âœ… **WebGPU Acceleration** - Hardware-accelerated training with GPU
- âœ… **GPU Metrics Dashboard** - Real-time performance monitoring
- âœ… **Automatic Fallback** - Seamless CPU fallback when GPU unavailable

ğŸ“š **[See full mathematical documentation â†’](./MATHEMATICAL_ENHANCEMENTS.md)**

> This repository is intentionally simple: the only thing your agent does is **train** and **update** the model JSON.  
> UI and code are entirely in English. The tokenizer is languageâ€‘agnostic.

---

## Quickstart

```bash
# 1) Install
pnpm i  # or: npm i / yarn

# 2) Dev
pnpm dev

# 3) Quality
pnpm lint && pnpm test

# 4) Build
pnpm build && pnpm preview

# 5) Train (Node script, no browser needed)
pnpm train
```

The browser UI allows you to paste a training corpus and interact with the model.  
The Node training script (`scripts/train.ts`) reads from `data/corpus.txt` and writes the artifact to `models/neuroâ€‘linguaâ€‘v324.json`.

---

## Repo layout

```
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ corpus.txt                  # training corpus for the agent
â”œâ”€â”€ models/
â”‚   â””â”€â”€ neuro-lingua-v324.json       # latest trained model artifact
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.ts                    # Node training script (ts-node)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib/ProNeuralLM.ts          # the neural LM (framework-free)
â”‚   â”œâ”€â”€ App.tsx                     # React UI (English)
â”‚   â””â”€â”€ main.tsx
â”œâ”€â”€ index.html
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ vite.config.ts
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .github/workflows/train-model.yml
```

---

## Agent / CI

- **Workflow**: `.github/workflows/train-model.yml`
- **Triggers**:
  - `workflow_dispatch` with inputs `epochs`, `optimizer`, `dropout`
  - `push` events that touch `data/corpus.txt`
- **Action**: install dependencies, run `pnpm train`, commit the updated JSON artifact (if changed) with the built-in `GITHUB_TOKEN`

Example manual dispatch:

```bash
gh workflow run train-model.yml \
  -f epochs=40 \
  -f optimizer=adam \
  -f dropout=0.15
```

Grant the workflow write access:

- Repository â†’ Settings â†’ Actions â†’ General â†’ Workflow permissions â†’ **Read and write permissions**

---

## âš ï¸ Important Warnings

### Privacy & Sensitive Data

**DO NOT use this application with:**

- Personally Identifiable Information (PII)
- Sensitive personal data
- Confidential business information
- Medical records or health data
- Financial information
- Authentication credentials or secrets

**Why?** This application stores training data and models in browser localStorage, which:

- Is not encrypted
- Persists across sessions
- Could be accessed by browser extensions or malicious scripts
- May be included in browser sync/backup

**Recommendation:** Use only public, non-sensitive text for training and experimentation.

---

## Notes

- The LM is educational and runs fully in the browser. It is **not** optimized for long texts.
- Browser sessions persist hyperparameters, tokenizer configuration, and training corpora via `localStorage`. Use the onboarding card in the UI to review import/export and pause/resume behaviour.
- Download the training-history CSV from the statistics panel to compare runs.
- The tokenizer uses a Unicode-aware rule by default. Override via environment when training headlessly: set `TOKENIZER_MODE=ascii` or provide `TOKENIZER_MODE=custom` with `TOKENIZER_PATTERN="[^a-z]+"`. The UI exposes the same presets and allows exporting/importing tokenizer JSON files.
- The Node training script now adds `<PAD>` to the vocabulary to match the browser experience.

Enjoy!
