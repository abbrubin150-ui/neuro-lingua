# Neuroâ€‘Lingua DOMESTICA â€” v4.3.0 (EN)

**Browserâ€‘native neural language model** built in React + TypeScript.

> Runtime version: **v4.3.0** (UI + model)Â Â· Governance/export spec documented in **CHANGELOG_v3.3.md**. The bilingual UI, project/run management, and WebGPU acceleration in `App.tsx` match this runtime.

ğŸŒ **[Try the live demo â†’](https://abbrubin150-ui.github.io/neuro-lingua/)** â€” includes an English â†” Hebrew toggle for the UI

## Core Features

- **Multiple Architectures**: Standard ProNeuralLM, AdvancedNeuralLM, and fully-functional Transformer models with multi-head attention
- **WebGPU Acceleration**: 2-5x faster training on compatible hardware with automatic CPU fallback (tested on Chromium-based browsers with recent drivers)
- **7 Optimizers**: SGD with Momentum, Adam, **Lion (v4.0)**, **Sophia (v4.3)**, Damped Newton, L-BFGS
- **Dropout** (trainâ€‘only)
- **Advanced Text Generation**: Greedy, Sampling (Top-p/Top-k/Typical/Mirostat v2), Beam Search, and Contrastive decoding
- **Session persistence**, onboarding tips, and downloadable **training-history CSVs** (localized labels)
- **Tokenizer presets** (Unicode/ASCII/custom) with import/export support
- **Agent** workflow: a single GitHub Action retrains the model and commits the updated JSON artifact

## ğŸš€ Advanced Features

### Neural Network Architectures

- **ğŸ”® Transformer**: Fully-implemented multi-head self-attention with position embeddings, **pre-attention/pre-FFN RMSNorm**, residual connections, and **Grouped-Query Attention (GQA)** for efficient KV caching (configurable layers and heads)
- **ğŸš€ AdvancedNeuralLM**: State-of-the-art feedforward architecture
- **ğŸ“Š ProNeuralLM**: Standard baseline model

### Mathematical Enhancements

- âœ… **He/Xavier Initialization** - Faster convergence with proper weight init
- âœ… **Advanced Activations** - LeakyReLU, ELU, GELU, Swish
- âœ… **Learning Rate Scheduling** - Cosine annealing, exponential decay, warmup
- âœ… **L2 Regularization** - Weight decay for better generalization
- âœ… **Layer Normalization** - Training stability
- âœ… **RMSNorm (v4.0)** - Efficient normalization (20% less memory, 2x faster)
- âœ… **Lion Optimizer (v4.0)** - Memory-efficient optimizer with faster convergence
- âœ… **Sophia Optimizer (v4.3)** - Second-order stochastic optimizer with 2Ã— faster convergence
- âœ… **Mixed Precision Training (v4.3)** - FP16/FP32 for 2-3Ã— speedup and 50% memory reduction
- âœ… **Numerical Stability** - Log-sum-exp, stable softmax
- âœ… **Perplexity Calculation** - Model evaluation metric

### Text Generation Methods

- âœ… **Greedy Decoding** - Deterministic selection of most likely token (argmax)
- âœ… **Temperature Sampling** - Controlled randomness in generation
- âœ… **Top-k Sampling** - Sample from k most likely tokens
- âœ… **Nucleus (Top-p) Sampling** - Sample from smallest set with cumulative probability p
- âœ… **Typical Sampling** - Entropy-based token selection for natural coherence
- âœ… **Mirostat v2 Sampling** - Adaptive sampling with dynamic perplexity control
- âœ… **Beam Search** - Maintain multiple hypotheses for higher quality output
- âœ… **Contrastive Search** - Balance model confidence with diversity to reduce repetition

### Performance Optimization

- âœ… **WebGPU Acceleration** - Hardware-accelerated training with GPU
- âœ… **GPU Metrics Dashboard** - Real-time performance monitoring
- âœ… **Automatic Fallback** - Seamless CPU fallback when GPU unavailable
- âœ… **Mixed Precision (v4.3)** - FP16/FP32 training with dynamic loss scaling

### ğŸ”€ Sparse Attention (v4.3)

Efficient attention patterns that reduce O(nÂ²) complexity to O(n) for longer sequences:

- âœ… **Local (Sliding Window)** - Each token attends to w neighbors
- âœ… **BigBird** - Local + Global + Random attention (Google Research)
- âœ… **Longformer** - Local + Global with task-specific tokens
- âœ… **Block Sparse** - Block-diagonal for hardware efficiency
- âœ… **Strided/Dilated** - Hierarchical attention patterns
- âœ… **Axial** - Factorized 2D attention for image-like data

**Benefits**: 2-4Ã— faster for long sequences (n > 512), enables 4k-16k+ token context windows in browser

#### WebGPU Browser Compatibility

| Browser | Version | Status |
|---------|---------|--------|
| Chrome | 113+ | âœ… Full support |
| Edge | 113+ | âœ… Full support |
| Opera | 99+ | âœ… Full support |
| Firefox | 127+ | âš ï¸ Behind flag (`dom.webgpu.enabled`) |
| Safari | â€” | âŒ Not yet supported |

> **Note:** WebGPU requires a compatible GPU and up-to-date drivers. If GPU acceleration is unavailable, Neuro-Lingua automatically falls back to CPU with full functionality (2-5x slower).

### ğŸ¯ Experiment Management (Î£-SIG Compliance)

- âœ… **Project & Run Architecture** - Full experiment tracking with frozen configurations
- âœ… **Decision Ledger** - Governance framework with rationale, witness, and expiry tracking
- âœ… **Scenario Testing** - Define and evaluate test scenarios across multiple runs
- âœ… **Execution Status** - EXECUTE/HOLD/ESCALATE compliance checks
- âœ… **Trace Export** - Complete audit trail with model weights, configs, and metadata

### ğŸ” Model Interpretability

- âœ… **SHAP Values** - Estimate feature importance using Shapley values
- âœ… **Integrated Gradients** - Attribution method for input token contributions
- âœ… **Attention Rollout** - Visualize attention flow in Transformer models
- âœ… **Explainability Panel** - Interactive UI for model interpretation

### ğŸ“Š Advanced Visualization

- âœ… **Embedding Visualization** - Interactive t-SNE and UMAP projections
- âœ… **Information Theory Panel** - I(X;Z) vs I(Z;Y) information plane
- âœ… **Information Bottleneck** - Compression-prediction trade-off analysis
- âœ… **Canvas Interaction** - Pan, zoom, and explore embedding spaces

### ğŸ§  Cerebro: Adaptive Neuron Injection

- âœ… **Bubble Detection** - Identify residual clusters in embedding space
- âœ… **Neuron Injection** - Dynamically grow hidden layer to capture missed patterns
- âœ… **Architecture Adapters** - Works with ProNeuralLM, AdvancedNeuralLM, and TransformerLM
- âœ… **Rollback Support** - Undo injection if validation loss increases
- âœ… **Decision Ledger Integration** - All injections logged for Î£-SIG compliance

### ğŸ—œï¸ Model Compression

- âœ… **Int8 Quantization** - 4x size reduction with minimal accuracy loss
- âœ… **Knowledge Distillation** - Train smaller student models from teacher
- âœ… **Low-Rank Approximation** - SVD-based weight compression with configurable rank
- âœ… **Compression Panel** - Interactive UI for compression operations

### ğŸ§¬ Brain Vitals System

- âœ… **Mood Tracking** - CALM, FOCUSED, AGITATED, DREAMY, BURNT_OUT states
- âœ… **Creativity & Stability Gauges** - 0-100 vitals based on usage
- âœ… **Event Diary** - Logs training, generation, and feeding events
- âœ… **Pet Naming** - Assign friendly labels to model instances
- âœ… **Telemetry Panel** - Detailed brain analytics and trends

### ğŸ“Š DAG-Based Causal Inference (v4.2)

- âœ… **CausalInferenceEngine** - Probabilistic dynamic causal inference system
- âœ… **Directed Acyclic Graph (DAG)** - Explicit causal structure representation
- âœ… **Temporal Dependencies** - Y_{t-1} â†’ Y_t autoregressive modeling
- âœ… **Unmeasured Confounders** - U-variable handling with sensitivity analysis
- âœ… **AIPW Estimator** - Augmented Inverse Probability Weighting for robust ATE
- âœ… **Adaptive Quantization** - Entropy-based dynamic discretization
- âœ… **Bias Verification** - Neutrality axiom and differential privacy checks
- âœ… **Three-Phase Pipeline** - Offline learning â†’ Online selection â†’ Statistical testing

### ğŸ“„ Neuro-Lingua Lite

- âœ… **Single-File HTML** - Complete neural LM in one portable HTML file
- âœ… **Zero Dependencies** - No build tools, frameworks, or external files
- âœ… **Mobile Friendly** - Responsive design for all screen sizes
- âœ… **Quick Start** - Just open `neuro-lingua-lite.html` in any browser

ğŸ“š **[See full mathematical documentation â†’](./MATHEMATICAL_ENHANCEMENTS.md)**
ğŸš€ **[v4.0 Mathematical & Architectural Upgrades (Hebrew) â†’](./NEURO_LINGUA_V4_UPGRADES.md)**
ğŸ“– **[Transformer architecture guide â†’](./TRANSFORMER_GUIDE.md)**
âš¡ **[GPU acceleration setup â†’](./GPU_ACCELERATION_GUIDE.md)**

> This repository is intentionally simple: the only thing your agent does is **train** and **update** the model JSON.
> The tokenizer is languageâ€‘agnostic, while the UI strings are available in English and Hebrew.

### Localization & translations

- Use the language toggle in the navbar to switch between English (LTR) and Hebrew (RTL) for the training editor, onboarding card, info cards, and chat console labels.
- Advanced configuration options, chart axes, and logs are currently English-only. Contributions that extend localization to those areas are welcome.
- Translations live directly in [`src/App.tsx`](./src/App.tsx) inside the `TRANSLATIONS` map so you can add new locales without touching other components.

---

## Quickstart

```bash
# 1) Install
pnpm i  # or: npm i / yarn

# 2) Dev
pnpm dev

# 3) Quality
pnpm lint && pnpm test

# 4) Build
pnpm build && pnpm preview

# 5) Train (Node script, no browser needed)
pnpm train

# 6) GPU Benchmarks (optional)
pnpm benchmark:gpu
```

The browser UI allows you to paste a training corpus and interact with the model.  
The Node training script (`scripts/train.ts`) reads from `data/corpus.txt` and writes the artifact to `models/neuroâ€‘linguaâ€‘v324.json`.

---

## Repo layout

```
.
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ ci.yml                      # Continuous integration (test, lint, build)
â”‚   â”œâ”€â”€ train-model.yml             # Automated model retraining
â”‚   â””â”€â”€ deploy-pages.yml            # GitHub Pages deployment
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ corpus.txt                  # training corpus for the agent
â”‚   â”œâ”€â”€ raw/                        # raw datasets (wikitext, hebrew_news)
â”‚   â””â”€â”€ processed/                  # preprocessed train/val/test splits
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ experiments/                # experiment results and summaries
â”‚   â”œâ”€â”€ theory/                     # theoretical documentation
â”‚   â””â”€â”€ visuals/                    # embedding visualization exports
â”œâ”€â”€ models/
â”‚   â””â”€â”€ neuro-lingua-v324.json      # latest trained model artifact (3MB)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.ts                    # Node training script (ts-node)
â”‚   â”œâ”€â”€ benchmark_gpu.ts            # GPU performance benchmarks
â”‚   â””â”€â”€ visualize_embeddings.ts     # Generate t-SNE/UMAP visualizations
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ webgpu.ts               # WebGPU backend and tensor operations
â”‚   â”‚   â”œâ”€â”€ gpu_neural_ops.ts       # High-level neural operations on GPU
â”‚   â”‚   â””â”€â”€ mixed_precision.ts      # FP16/FP32 mixed precision training
â”‚   â”œâ”€â”€ compression/
â”‚   â”‚   â”œâ”€â”€ compress.ts             # Unified compression interface
â”‚   â”‚   â”œâ”€â”€ quantization.ts         # Int8 weight quantization
â”‚   â”‚   â”œâ”€â”€ distillation.ts         # Knowledge distillation
â”‚   â”‚   â””â”€â”€ lowrank.ts              # SVD-based low-rank approximation
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ TrainingPanel.tsx       # Main training configuration panel
â”‚   â”‚   â”œâ”€â”€ ModelMetrics.tsx        # Performance metrics dashboard
â”‚   â”‚   â”œâ”€â”€ ProjectManager.tsx      # Project/run management (Î£-SIG)
â”‚   â”‚   â”œâ”€â”€ ScenarioManager.tsx     # Test scenario editor
â”‚   â”‚   â”œâ”€â”€ DecisionLedgerEditor.tsx # Governance/decision tracking
â”‚   â”‚   â”œâ”€â”€ ExplainabilityPanel.tsx # SHAP/gradients/attention visualization
â”‚   â”‚   â”œâ”€â”€ EmbeddingVisualizationPanel.tsx # t-SNE/UMAP interactive canvas
â”‚   â”‚   â”œâ”€â”€ InformationTheoryPanel.tsx # Information bottleneck metrics
â”‚   â”‚   â”œâ”€â”€ CompressionPanel.tsx    # Model compression UI
â”‚   â”‚   â”œâ”€â”€ BrainPanel.tsx          # Brain vitals and mood tracking
â”‚   â”‚   â”œâ”€â”€ CerebroPanel.tsx        # Cerebro neuron injection UI
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx       # Chat-style generation UI
â”‚   â”‚   â””â”€â”€ TokenizerConfig.tsx     # Tokenizer settings
â”‚   â”œâ”€â”€ contexts/
â”‚   â”‚   â””â”€â”€ ProjectContext.tsx      # Project/run state management
â”‚   â”œâ”€â”€ explainability/
â”‚   â”‚   â”œâ”€â”€ shap.ts                 # SHAP values
â”‚   â”‚   â”œâ”€â”€ integrated_gradients.ts # Integrated gradients
â”‚   â”‚   â””â”€â”€ attention_rollout.ts    # Attention visualization
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ sampler.ts              # Top-k, top-p, temperature sampling
â”‚   â”‚   â”œâ”€â”€ beam_search.ts          # Beam search implementation
â”‚   â”‚   â””â”€â”€ contrastive_search.ts   # Contrastive decoding
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ ProNeuralLM.ts          # Base feedforward LM
â”‚   â”‚   â”œâ”€â”€ AdvancedNeuralLM.ts     # Enhanced LM with advanced features
â”‚   â”‚   â”œâ”€â”€ TransformerLM.ts        # Transformer architecture
â”‚   â”‚   â”œâ”€â”€ MathUtils.ts            # Numerical stability utilities
â”‚   â”‚   â”œâ”€â”€ BrainEngine.ts          # Brain vitals and mood system
â”‚   â”‚   â”œâ”€â”€ GovernanceEngine.ts     # Autonomous parameter calibration
â”‚   â”‚   â”œâ”€â”€ CausalInferenceEngine.ts # DAG-based causal inference system
â”‚   â”‚   â”œâ”€â”€ RMSNorm.ts              # Root Mean Square Normalization
â”‚   â”‚   â”œâ”€â”€ storage.ts              # localStorage abstraction
â”‚   â”‚   â”œâ”€â”€ utils.ts                # Tokenizer and CSV utilities
â”‚   â”‚   â”œâ”€â”€ traceExport.ts          # Î£-SIG compliant experiment tracing
â”‚   â”‚   â””â”€â”€ expandable/             # Cerebro neuron injection system
â”‚   â”‚       â”œâ”€â”€ InjectionEngine.ts  # Diagnostics, proposals, execution
â”‚   â”‚       â”œâ”€â”€ InjectableLayer.ts  # Minimal interface for injection
â”‚   â”‚       â”œâ”€â”€ ProNeuralLMAdapter.ts
â”‚   â”‚       â”œâ”€â”€ AdvancedNeuralLMAdapter.ts
â”‚   â”‚       â”œâ”€â”€ TransformerLMAdapter.ts
â”‚   â”‚       â”œâ”€â”€ bubbleExtractor.ts  # Extract bubbles from embeddings
â”‚   â”‚       â””â”€â”€ injection_math.ts   # Residual analysis, eigenvectors
â”‚   â”œâ”€â”€ losses/                     # Advanced loss functions
â”‚   â”œâ”€â”€ math/
â”‚   â”‚   â”œâ”€â”€ causal_math.ts          # Causal inference mathematics
â”‚   â”‚   â”œâ”€â”€ dag_operations.ts       # DAG graph operations
â”‚   â”‚   â”œâ”€â”€ analysis.ts             # Spectral/Lyapunov analysis
â”‚   â”‚   â””â”€â”€ statistics.ts           # Fisher information statistics
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ sparse_attention.ts     # Sparse attention patterns (BigBird, Longformer, etc.)
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ LionOptimizer.ts        # Lion optimizer (v4.0)
â”‚   â”‚   â””â”€â”€ SophiaOptimizer.ts      # Sophia second-order optimizer (v4.3)
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”œâ”€â”€ causal.ts               # Causal inference types
â”‚   â”‚   â””â”€â”€ dag.ts                  # DAG structure types
â”‚   â”œâ”€â”€ visualization/              # Embedding visualization (t-SNE, UMAP)
â”‚   â”œâ”€â”€ App.tsx                     # Main React application
â”‚   â””â”€â”€ main.tsx                    # Application entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ ProNeuralLM.test.ts         # Core model tests
â”‚   â”œâ”€â”€ AdvancedNeuralLM.test.ts    # Advanced features tests
â”‚   â”œâ”€â”€ TransformerLM.test.ts       # Transformer tests
â”‚   â””â”€â”€ numerics/                   # Numerical correctness tests
â”œâ”€â”€ index.html
â”œâ”€â”€ neuro-lingua-lite.html          # Standalone lightweight version
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ vite.config.ts
â”œâ”€â”€ CLAUDE.md                       # AI assistant development guide
â”œâ”€â”€ MATHEMATICAL_ENHANCEMENTS.md    # Detailed math formulations
â”œâ”€â”€ TRANSFORMER_GUIDE.md            # Transformer architecture explanation
â”œâ”€â”€ GPU_ACCELERATION_GUIDE.md       # WebGPU setup and usage
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## Agent / CI

- **Workflow**: `.github/workflows/train-model.yml`
- **Triggers**:
  - Manual `workflow_dispatch` inputs for `epochs`, `optimizer`, `dropout`
  - `push` events that touch `data/corpus.txt` or `scripts/train.ts`
- **Steps**:
  1. Checkout the repo with history so model diffs can be detected.
  2. Install dependencies via `pnpm install --frozen-lockfile`.
  3. Run `pnpm train` with those inputs provided as `EPOCHS`, `OPTIMIZER`, `DROPOUT` environment variables.
  4. Commit and push `models/neuro-lingua-v324.json` if the artifact changed.

Because the workflow pushes commits, the token it runs with must have `contents: write` access:

- Repository â†’ Settings â†’ Actions â†’ General â†’ Workflow permissions â†’ **Read and write permissions**
- For forks, supply a PAT (for example `secrets.WORKFLOW_TOKEN`) and configure the job to use it before `git push`.

Example manual dispatch:

```bash
gh workflow run train-model.yml \
  -f epochs=40 \
  -f optimizer=adam \
  -f dropout=0.15
```

---

## âš ï¸ Important Warnings

### Privacy & Sensitive Data

**DO NOT use this application with:**

- Personally Identifiable Information (PII)
- Sensitive personal data
- Confidential business information
- Medical records or health data
- Financial information
- Authentication credentials or secrets

**Why?** This application stores training data and models in browser localStorage, which:

- Is not encrypted
- Persists across sessions
- Could be accessed by browser extensions or malicious scripts
- May be included in browser sync/backup

**Recommendation:** Use only public, non-sensitive text for training and experimentation.

---

## ğŸ¯ Experiment Management with Î£-SIG Compliance

Neuro-Lingua implements the **Î£-SIG (Scientific Infrastructure for Governance)** framework for reproducible experiment tracking:

### Projects & Runs

- **Projects** organize related training experiments with shared goals
- **Runs** capture frozen training configurations with complete snapshots:
  - All hyperparameters (frozen and immutable after creation)
  - Architecture configuration (ProNeuralLM/AdvancedNeuralLM/TransformerLM)
  - Tokenizer settings
  - Training corpus with checksum
  - Complete training history and results
  - Serialized model weights

### Decision Ledger

Every run includes a governance layer with:

- **Rationale**: Why this training run is necessary
- **Witness**: Who authorized the training (e.g., "local-user")
- **Expiry**: Optional expiration date (ISO 8601)
- **Rollback**: Action after expiry (keep/delete-after-expiry/archive)
- **Execution Status**: EXECUTE âœ… / HOLD â¸ï¸ / ESCALATE ğŸš¨

### Scenario Testing

Define test scenarios per project:

- **Prompt**: Input text for generation
- **Expected Response**: Optional reference output
- **Scoring**: Track performance across multiple runs
- **Comparison**: Evaluate which configuration performs best

### Trace Export

Export models with complete audit trail including project metadata, decision ledger, training trace, and full reproducibility information. The current runtime (v4.2.0) emits the v3.3 governance/export fields described in `CHANGELOG_v3.3.md`.

---

## ğŸ” Model Interpretability & Visualization

### Explainability Methods

- **SHAP Values**: Estimate token importance using Shapley value approximation
- **Integrated Gradients**: Attribution method measuring token contributions
- **Attention Rollout**: Visualize attention flow in Transformer models (multi-head support)

### Embedding Visualization

- **t-SNE Projection**: Interactive 2D visualization of token embeddings
- **UMAP Projection**: Alternative dimensionality reduction with configurable parameters
- **Canvas Interaction**: Pan, zoom, and explore embedding spaces
- **Export**: Save visualizations for documentation

### Information Theory

- **Information Bottleneck**: I(X;Z) vs I(Z;Y) information plane visualization
- **Compression-Prediction Trade-off**: Balance between model compression and prediction accuracy
- **Entropy Metrics**: H(Z), H(Z|X) tracking during training

---

## ğŸ“š Documentation

- **[CLAUDE.md](./CLAUDE.md)** - Comprehensive AI assistant development guide
- **[MATHEMATICAL_ENHANCEMENTS.md](./MATHEMATICAL_ENHANCEMENTS.md)** - Detailed mathematical formulations
- **[NEURO_LINGUA_V4_UPGRADES.md](./NEURO_LINGUA_V4_UPGRADES.md)** - v4.0 Mathematical & Architectural Upgrades (Hebrew)
- **[TRANSFORMER_GUIDE.md](./TRANSFORMER_GUIDE.md)** - Transformer architecture deep dive
- **[TRANSFORMER_IMPLEMENTATION.md](./TRANSFORMER_IMPLEMENTATION.md)** - Implementation details
- **[GPU_ACCELERATION_GUIDE.md](./GPU_ACCELERATION_GUIDE.md)** - WebGPU setup and benchmarking
- **[DEVELOPMENT_SETUP_GUIDE.md](./DEVELOPMENT_SETUP_GUIDE.md)** - Development environment setup
- **[CHANGELOG_v3.3.md](./CHANGELOG_v3.3.md)** - Project & Run management release notes

---

## Notes

- The LM is educational and runs fully in the browser. It is **not** optimized for long texts.
- Browser sessions persist hyperparameters, tokenizer configuration, and training corpora via `localStorage`. Use the onboarding card in the UI to review import/export and pause/resume behaviour.
- Download the training-history CSV from the statistics panel to compare runs.
- The tokenizer uses a Unicode-aware rule by default. Override via environment when training headlessly: set `TOKENIZER_MODE=ascii` or provide `TOKENIZER_MODE=custom` with `TOKENIZER_PATTERN="[^a-z]+"`. The UI exposes the same presets and allows exporting/importing tokenizer JSON files.
- The Node training script now adds `<PAD>` to the vocabulary to match the browser experience.

Enjoy!
