{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability Playbook\n",
    "This notebook demonstrates how the new `src/explainability` utilities support\n",
    "Shapley-value style attributions, Integrated Gradients, and attention\n",
    "rollout-based visualisation. Each section mirrors the TypeScript API with a\n",
    "small NumPy/PyTorch-free prototype so that the conceptual steps remain clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SHAP via permutation sampling\n",
    "We approximate Shapley values by randomly permuting features and accumulating\n",
    "marginal contributions. The code below mirrors\n",
    "`estimateShapValues` from `src/explainability/shap.ts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable, List\n",
    "import random\n",
    "\n",
    "def logistic_model(weights: List[float], bias: float) -> Callable[[List[float]], float]:\n",
    "    def _forward(x: List[float]) -> float:\n",
    "        z = sum(w * xi for w, xi in zip(weights, x)) + bias\n",
    "        return 1.0 / (1.0 + math.exp(-z))\n",
    "    return _forward\n",
    "\n",
    "def shap_permutation(model: Callable[[List[float]], float],\n",
    "                     x: List[float],\n",
    "                     baseline: List[float],\n",
    "                     permutations: int = 64,\n",
    "                     seed: int = 7) -> List[float]:\n",
    "    rng = random.Random(seed)\n",
    "    contrib = [0.0 for _ in x]\n",
    "    for _ in range(permutations):\n",
    "        order = list(range(len(x)))\n",
    "        rng.shuffle(order)\n",
    "        current = baseline.copy()\n",
    "        prev = model(current)\n",
    "        for idx in order:\n",
    "            current[idx] = x[idx]\n",
    "            nxt = model(current)\n",
    "            contrib[idx] += nxt - prev\n",
    "            prev = nxt\n",
    "    return [c / permutations for c in contrib]\n",
    "\n",
    "model = logistic_model([1.4, -0.8], bias=-0.2)\n",
    "x = [0.9, -1.1]\n",
    "baseline = [0.0, 0.0]\n",
    "shap_vals = shap_permutation(model, x, baseline, permutations=128)\n",
    "prediction = model(x)\n",
    "baseline_pred = model(baseline)\n",
    "total_contrib = sum(shap_vals)\n",
    "prediction, baseline_pred, total_contrib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Integrated Gradients along a straight path\n",
    "Integrated Gradients requires access to the model's gradient. We craft an\n",
    "analytic gradient for the logistic regression above and numerically integrate\n",
    "along the path between a baseline and the target input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient(weights: List[float], bias: float) -> Callable[[List[float]], List[float]]:\n",
    "    def _grad(x: List[float]) -> List[float]:\n",
    "        z = sum(w * xi for w, xi in zip(weights, x)) + bias\n",
    "        p = 1.0 / (1.0 + math.exp(-z))\n",
    "        return [w * p * (1 - p) for w in weights]\n",
    "    return _grad\n",
    "\n",
    "def integrated_gradients(grad_fn: Callable[[List[float]], List[float]],\n",
    "                          baseline: List[float],\n",
    "                          input_x: List[float],\n",
    "                          steps: int = 50) -> List[float]:\n",
    "    acc = [0.0 for _ in input_x]\n",
    "    for step in range(1, steps + 1):\n",
    "        alpha = step / steps\n",
    "        point = [b + alpha * (xi - b) for b, xi in zip(baseline, input_x)]\n",
    "        grad = grad_fn(point)\n",
    "        for i, g in enumerate(grad):\n",
    "            acc[i] += g\n",
    "    return [(xi - b) * acc_i / steps for (xi, b), acc_i in zip(zip(input_x, baseline), acc)]\n",
    "\n",
    "grad_fn = logistic_gradient([1.4, -0.8], bias=-0.2)\n",
    "ig_vals = integrated_gradients(grad_fn, baseline, x, steps=100)\n",
    "ig_vals, sum(ig_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention rollout\n",
    "We combine a stack of attention matrices (averaged over heads) using matrix\n",
    "multiplication. Residual connections correspond to injecting an identity\n",
    "matrix before multiplying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout(layers, add_identity=True):\n",
    "    size = len(layers[0])\n",
    "    def _normalize(matrix):\n",
    "        return [[value / max(1e-9, sum(row)) for value in row] for row in matrix]\n",
    "    result = [[1 if i == j else 0 for j in range(size)] for i in range(size)] if add_identity else layers[0]\n",
    "    if add_identity:\n",
    "        result = multiply(result, _normalize(layers[0]))\n",
    "        start = 1\n",
    "    else:\n",
    "        result = _normalize(result)\n",
    "        start = 1\n",
    "    for layer in layers[start:]:\n",
    "        result = multiply(result, _normalize(layer))\n",
    "    return result\n",
    "\n",
    "def multiply(a, b):\n",
    "    size = len(a)\n",
    "    out = [[0.0 for _ in range(size)] for _ in range(size)]\n",
    "    for i in range(size):\n",
    "        for k in range(size):\n",
    "            for j in range(size):\n",
    "                out[i][j] += a[i][k] * b[k][j]\n",
    "    return out\n",
    "\n",
    "layer1 = [[0.6, 0.4, 0.0], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7]]\n",
    "layer2 = [[0.5, 0.3, 0.2], [0.1, 0.8, 0.1], [0.3, 0.2, 0.5]]\n",
    "rollout_matrix = attention_rollout([layer1, layer2])\n",
    "rollout_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
